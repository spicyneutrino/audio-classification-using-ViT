{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf4b3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ad7e83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "def download_and_extract_zip(url: str, destination: str, remove_source: bool = True) -> Path:\n",
    "\n",
    "    data_folder = Path('data/')\n",
    "    destination_path = data_folder / destination\n",
    "\n",
    "    if data_folder.is_dir():\n",
    "        print(f\"Data folder {data_folder} already exists.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Downloading data from {url} to {data_folder}\")\n",
    "        destination_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        target_file = Path(url).name\n",
    "\n",
    "        with open(data_folder / target_file, 'wb') as f:\n",
    "            response = requests.get(url)\n",
    "            f.write(response.content)\n",
    "        \n",
    "        with zipfile.ZipFile(data_folder / target_file, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(data_folder)\n",
    "        \n",
    "        if remove_source:\n",
    "            os.remove(data_folder / target_file)\n",
    "    \n",
    "    return destination_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0a67f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from datasets import load_dataset\n",
    "except:\n",
    "    %pip -q install datasets\n",
    "    from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"danavery/urbansound8K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d16b8b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: dict_keys(['train'])\n"
     ]
    }
   ],
   "source": [
    "available_splits = ds.keys()\n",
    "print(f\"Available splits: {available_splits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "13f1878a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None),\n",
       " 'slice_file_name': Value(dtype='string', id=None),\n",
       " 'fsID': Value(dtype='int64', id=None),\n",
       " 'start': Value(dtype='float64', id=None),\n",
       " 'end': Value(dtype='float64', id=None),\n",
       " 'salience': Value(dtype='int64', id=None),\n",
       " 'fold': Value(dtype='int64', id=None),\n",
       " 'classID': Value(dtype='int64', id=None),\n",
       " 'class': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "330d9d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 8732\n",
      "Fold column exists. Splitting dataset into predefined folds.\n",
      "Train dataset size: 7079\n",
      "Validation dataset size: 816\n",
      "Test dataset size: 837\n",
      "Data split successfully.\n"
     ]
    }
   ],
   "source": [
    "if \"train\" in ds.keys():\n",
    "    full_dataset = ds[\"train\"]\n",
    "    print(f\"Full dataset size: {len(full_dataset)}\")\n",
    "\n",
    "    if \"fold\" in full_dataset.features:\n",
    "        print(\"Fold column exists. Splitting dataset into predefined folds.\")\n",
    "\n",
    "        train_dataset = full_dataset.filter(lambda x: x[\"fold\"] <= 8)\n",
    "        val_dataset = full_dataset.filter(lambda x: x[\"fold\"] == 9)\n",
    "        test_dataset = full_dataset.filter(lambda x: x[\"fold\"] == 10)\n",
    "\n",
    "        print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "        print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "        print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "        if len(train_dataset) + len(val_dataset) + len(test_dataset) == len(\n",
    "            full_dataset\n",
    "        ):\n",
    "            print(\"Data split successfully.\")\n",
    "        else:\n",
    "            print(\n",
    "                \"ERROR: Sum of split doesn't match the full dataset. Check the folds again.\"\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: 'fold' column not found. Performing a random split.\")\n",
    "\n",
    "        # Split full dataset\n",
    "        # Training : 80%\n",
    "        # Temp Test: 20%\n",
    "        train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "        train_dataset = train_test_split[\"train\"]\n",
    "        test_temp_dataset = train_test_split[\"test\"]\n",
    "\n",
    "        # Split temp test dataset\n",
    "        # val_dataset: 50%\n",
    "        # test_dataset: 50%\n",
    "        val_test_split = test_temp_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "        val_dataset = val_test_split[\"train\"]\n",
    "        test_dataset = val_test_split[\"test\"]\n",
    "\n",
    "        print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "        print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "        print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: 'train' split not found. Please check the dataset.\")\n",
    "    train_dataset = None\n",
    "    val_dataset = None\n",
    "    test_dataset = None\n",
    "    print(\"The dataset has \", ds.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6ae883c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 7079\n",
      "Validation dataset size: 816\n",
      "Test dataset size: 837\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '100032-3-0-0.wav',\n",
       "  'array': array([-0.00454712, -0.00483704, -0.00460815, ..., -0.00065613,\n",
       "         -0.00048828,  0.        ], shape=(14004,)),\n",
       "  'sampling_rate': 44100},\n",
       " 'slice_file_name': '100032-3-0-0.wav',\n",
       " 'fsID': 100032,\n",
       " 'start': 0.0,\n",
       " 'end': 0.317551,\n",
       " 'salience': 1,\n",
       " 'fold': 5,\n",
       " 'classID': 3,\n",
       " 'class': 'dog_bark'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "326663bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as TV\n",
    "import numpy as np\n",
    "\n",
    "# 1. Preprocessing function\n",
    "# parameters\n",
    "N_MELS = 128\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "VIT_INPUT_SIZE = (224, 224)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "FREQ_MASK_PARAM = 80  # for SpecAugment\n",
    "TIME_MASK_PARAM = 100  # for SpecAugment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectrogram Calculation\n",
    "mel_spectrogram_transform = T.MelSpectrogram(\n",
    "    sample_rate=TARGET_SAMPLE_RATE,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    n_mels=N_MELS,\n",
    "    power=2.0,\n",
    ")\n",
    "\n",
    "amplitude_to_db_transform = T.AmplitudeToDB(stype=\"power\", top_db=80.0)\n",
    "\n",
    "# SpecAugment (Frequency and Time Masking)\n",
    "freq_mask_transform = T.FrequencyMasking(freq_mask_param=30)\n",
    "time_mask_transform = T.TimeMasking(time_mask_param=40)\n",
    "\n",
    "\n",
    "# # transforms to make it compatible with vision models\n",
    "# custom transform to handle channels\n",
    "class HandleChannels(nn.Module):\n",
    "    def forward(self, spec: torch.Tensor) -> torch.Tensor:\n",
    "        # Input shape: [1, n_mels, time_steps]\n",
    "        # Output shape: [3, n_mels, time_steps]\n",
    "        if spec.ndim == 2:\n",
    "            # If input is 2D, add a channel dimension\n",
    "            spec = spec.unsqueeze(0)\n",
    "        # format now: [1, n_mels, time_steps]\n",
    "        # Repeat across channel dimension to get 3 channels (mimicking RGB)\n",
    "        # If input has 1 channel, repeat to 3 channels\n",
    "        if spec.shape[0] == 1:\n",
    "            spec = spec.repeat(3, 1, 1)\n",
    "        # Shape after: [3, n_mels, time_steps]\n",
    "        return spec\n",
    "\n",
    "\n",
    "handle_channels_transform = HandleChannels()\n",
    "\n",
    "# Vision transforms\n",
    "resize_transform = TV.Resize(VIT_INPUT_SIZE, antialias=True)\n",
    "normalize_transform = TV.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "# Create the transform pipeline\n",
    "\n",
    "eval_transforms = TV.Compose(\n",
    "    [\n",
    "        mel_spectrogram_transform,\n",
    "        amplitude_to_db_transform,\n",
    "        handle_channels_transform,\n",
    "        resize_transform,\n",
    "        normalize_transform,\n",
    "    ]\n",
    ")\n",
    "\n",
    "training_transforms = TV.Compose(\n",
    "    [\n",
    "        mel_spectrogram_transform,\n",
    "        amplitude_to_db_transform,\n",
    "        # These transforms expect (..., freq, time)\n",
    "        freq_mask_transform,\n",
    "        time_mask_transform,\n",
    "        # Image transforms\n",
    "        handle_channels_transform,\n",
    "        resize_transform,\n",
    "        normalize_transform,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a8b8fc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '100032-3-0-0.wav',\n",
       "  'array': array([-0.00454712, -0.00483704, -0.00460815, ..., -0.00065613,\n",
       "         -0.00048828,  0.        ], shape=(14004,)),\n",
       "  'sampling_rate': 44100},\n",
       " 'slice_file_name': '100032-3-0-0.wav',\n",
       " 'fsID': 100032,\n",
       " 'start': 0.0,\n",
       " 'end': 0.317551,\n",
       " 'salience': 1,\n",
       " 'fold': 5,\n",
       " 'classID': 3,\n",
       " 'class': 'dog_bark'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cc347133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0045, -0.0048, -0.0046,  ..., -0.0007, -0.0005,  0.0000],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(train_dataset[0][\"audio\"][\"array\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7cb4c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(is_training: bool):\n",
    "\n",
    "    processor = training_transforms if is_training else eval_transforms\n",
    "\n",
    "    def preprocess_audio_to_spectrogram(sample):\n",
    "        print(\"Transform called with sample['audio'] type:\", type(sample[\"audio\"]))\n",
    "\n",
    "        audio_data = sample[\"audio\"]\n",
    "        if isinstance(audio_data, dict) and \"array\" in audio_data:\n",
    "            waveform = torch.from_numpy(audio_data[\"array\"]).float()\n",
    "            sample_rate = audio_data[\"sampling_rate\"]\n",
    "            label = (\n",
    "                torch.tensor(sample[\"classID\"], dtype=torch.long)\n",
    "                if \"classID\" in sample\n",
    "                else sample[\"label\"]\n",
    "            )\n",
    "            processed_spectrogram = processor(waveform)\n",
    "            result = dict(sample)\n",
    "            result[\"pixel_values\"] = processed_spectrogram\n",
    "            result[\"label\"] = label\n",
    "            return result\n",
    "\n",
    "        # Batch: list of dicts\n",
    "        elif (\n",
    "            isinstance(audio_data, list)\n",
    "            and isinstance(audio_data[0], dict)\n",
    "            and \"array\" in audio_data[0]\n",
    "        ):\n",
    "            pixel_values = []\n",
    "            labels = []\n",
    "            for i, audio_dict in enumerate(audio_data):\n",
    "                waveform = torch.from_numpy(audio_dict[\"array\"]).float()\n",
    "                sample_rate = audio_dict[\"sampling_rate\"]\n",
    "                if sample_rate != TARGET_SAMPLE_RATE:\n",
    "                    resampler = T.Resample(\n",
    "                        orig_freq=sample_rate, new_freq=TARGET_SAMPLE_RATE\n",
    "                    )\n",
    "                    waveform = resampler(waveform)\n",
    "                if waveform.ndim > 1 and waveform.shape[0] > 1:\n",
    "                    waveform = torch.mean(waveform, dim=0)\n",
    "                if waveform.ndim == 0:\n",
    "                    waveform = waveform.unsqueeze(0)\n",
    "                processed = processor(waveform)\n",
    "                pixel_values.append(processed)\n",
    "                # Handle label for batch\n",
    "                if \"classID\" in sample and isinstance(sample[\"classID\"], list):\n",
    "                    labels.append(sample[\"classID\"][i])\n",
    "                elif \"label\" in sample and isinstance(sample[\"label\"], list):\n",
    "                    labels.append(sample[\"label\"][i])\n",
    "                else:\n",
    "                    # fallback: try to get label from audio_dict if present\n",
    "                    labels.append(\n",
    "                        audio_dict.get(\"classID\", audio_dict.get(\"label\", -1))\n",
    "                    )\n",
    "            result = dict(sample)\n",
    "            result[\"pixel_values\"] = pixel_values\n",
    "            result[\"label\"] = labels\n",
    "            return result\n",
    "        else:\n",
    "            raise TypeError(f\"Unexpected type for sample['audio']: {type(audio_data)}\")\n",
    "\n",
    "        # Resample to target sample rate\n",
    "        if sample_rate != TARGET_SAMPLE_RATE:\n",
    "            resampler = T.Resample(orig_freq=sample_rate, new_freq=TARGET_SAMPLE_RATE)\n",
    "            waveform = resampler(waveform)\n",
    "            sample_rate = TARGET_SAMPLE_RATE\n",
    "\n",
    "        # Convert stereo to mono\n",
    "        if waveform.ndim > 1 and waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0)\n",
    "\n",
    "        # Ensure waveform is 1D for MelSpectrogram if it became 0D after mean\n",
    "        if waveform.ndim == 0:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "\n",
    "        # Apply the training transforms\n",
    "        processed_spectrogram = processor(waveform)\n",
    "        print(f\"Type after processor: {type(processed_spectrogram)}\")  # Debug\n",
    "\n",
    "        # return {\n",
    "        #     \"pixel_values\": processed_spectrogram,\n",
    "        #     \"label\": label,\n",
    "        # }\n",
    "        result = dict(sample)  # copy all original keys\n",
    "        result[\"pixel_values\"] = processed_spectrogram\n",
    "        result[\"label\"] = label\n",
    "        return result\n",
    "\n",
    "    return preprocess_audio_to_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "85f60bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transforms to datasets...\n",
      "Sample before transform: {'audio': {'path': '100032-3-0-0.wav', 'array': array([-0.00454712, -0.00483704, -0.00460815, ..., -0.00065613,\n",
      "       -0.00048828,  0.        ], shape=(14004,)), 'sampling_rate': 44100}, 'slice_file_name': '100032-3-0-0.wav', 'fsID': 100032, 'start': 0.0, 'end': 0.317551, 'salience': 1, 'fold': 5, 'classID': 3, 'class': 'dog_bark'}\n",
      "Transforms applied successfully.\n",
      "Transform called with sample['audio'] type: <class 'list'>\n",
      "Processed sample shape: torch.Size([3, 224, 224])\n",
      "Processed sample label: 3\n",
      "Processed spectrogram type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "train_transform_fn = preprocess_data(is_training=True)\n",
    "val_transform_fn = preprocess_data(is_training=False)\n",
    "\n",
    "print(\"Applying transforms to datasets...\")\n",
    "try:\n",
    "    train_exists = \"train_dataset\" in locals() and train_dataset is not None\n",
    "    val_exists = \"val_dataset\" in locals() and val_dataset is not None\n",
    "    test_exists = \"test_dataset\" in locals() and test_dataset is not None\n",
    "\n",
    "    print(\"Sample before transform:\", train_dataset[0])\n",
    "    if train_exists:\n",
    "        train_dataset.set_transform(train_transform_fn)\n",
    "    if val_exists:\n",
    "        val_dataset.set_transform(val_transform_fn)\n",
    "    if test_exists:\n",
    "        test_dataset.set_transform(val_transform_fn)\n",
    "\n",
    "    if train_exists or val_exists or test_exists:\n",
    "        print(\"Transforms applied successfully.\")\n",
    "    else:\n",
    "        print(\"No datasets found to apply transforms.\")\n",
    "\n",
    "    if train_exists:\n",
    "        processed_sample = train_dataset[0]\n",
    "        print(f\"Processed sample shape: {processed_sample['pixel_values'].shape}\")\n",
    "        print(f\"Processed sample label: {processed_sample['label']}\")\n",
    "        print(f\"Processed spectrogram type: {processed_sample['pixel_values'].dtype}\")\n",
    "# ...existing code...\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error applying transforms: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ab11872f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of train_dataset: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Transform called with sample['audio'] type: <class 'list'>\n",
      "Type of train_dataset[0]: <class 'dict'>\n",
      "train_dataset[0]: {'audio': {'path': '100032-3-0-0.wav', 'array': array([-0.00454712, -0.00483704, -0.00460815, ..., -0.00065613,\n",
      "       -0.00048828,  0.        ], shape=(14004,)), 'sampling_rate': 44100}, 'slice_file_name': '100032-3-0-0.wav', 'fsID': 100032, 'start': 0.0, 'end': 0.317551, 'salience': 1, 'fold': 5, 'classID': 3, 'class': 'dog_bark', 'pixel_values': tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 'label': 3}\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of train_dataset:\", type(train_dataset))\n",
    "first = train_dataset[0]\n",
    "print(\"Type of train_dataset[0]:\", type(first))\n",
    "print(\"train_dataset[0]:\", first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0bccfaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataloader size: 222\n",
      "Validation dataloader size: 26\n",
      "Test dataloader size: 27\n"
     ]
    }
   ],
   "source": [
    "# Crate DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "print(f\"Train dataloader size: {len(train_dataloader)}\")\n",
    "print(f\"Validation dataloader size: {len(val_dataloader)}\")\n",
    "print(f\"Test dataloader size: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "93c59646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape              Param #                   Trainable\n",
       "================================================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]          [1, 1000]                 768                       True\n",
       "├─Conv2d (conv_proj)                                         [1, 3, 224, 224]          [1, 768, 14, 14]          590,592                   True\n",
       "├─Encoder (encoder)                                          [1, 197, 768]             [1, 197, 768]             151,296                   True\n",
       "│    └─Dropout (dropout)                                     [1, 197, 768]             [1, 197, 768]             --                        --\n",
       "│    └─Sequential (layers)                                   [1, 197, 768]             [1, 197, 768]             --                        True\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    └─LayerNorm (ln)                                        [1, 197, 768]             [1, 197, 768]             1,536                     True\n",
       "├─Sequential (heads)                                         [1, 768]                  [1, 1000]                 --                        True\n",
       "│    └─Linear (head)                                         [1, 768]                  [1, 1000]                 769,000                   True\n",
       "================================================================================================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 173.23\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 232.27\n",
       "Estimated Total Size (MB): 336.96\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchinfo import summary\n",
    "# Create model\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "model = torchvision.models.vit_b_16(weights=weights)\n",
    "\n",
    "summary(model, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "443e8e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: encoder, Module: Encoder(\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): Sequential(\n",
      "    (encoder_layer_0): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_1): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_2): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_3): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_4): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_5): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_6): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_7): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_8): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_9): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_10): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_11): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "Layer: heads, Module: Sequential(\n",
      "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for layer in model.parameters():\n",
    "    layer.requires_grad = False\n",
    "\n",
    "for name, module in list(model.named_children())[-2:]:\n",
    "    print(f\"Layer: {name}, Module: {module}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "52fedc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aa7cc582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape              Trainable\n",
       "=======================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]          [1, 10]                   Partial\n",
       "├─Conv2d (conv_proj)                                         [1, 3, 224, 224]          [1, 768, 14, 14]          False\n",
       "├─Encoder (encoder)                                          [1, 197, 768]             [1, 197, 768]             False\n",
       "│    └─Dropout (dropout)                                     [1, 197, 768]             [1, 197, 768]             --\n",
       "│    └─Sequential (layers)                                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]             [1, 197, 768]             False\n",
       "│    └─LayerNorm (ln)                                        [1, 197, 768]             [1, 197, 768]             False\n",
       "├─Sequential (heads)                                         [1, 768]                  [1, 10]                   True\n",
       "│    └─Dropout (0)                                           [1, 768]                  [1, 768]                  --\n",
       "│    └─Linear (1)                                            [1, 768]                  [1, 10]                   True\n",
       "=======================================================================================================================================\n",
       "Total params: 85,806,346\n",
       "Trainable params: 7,690\n",
       "Non-trainable params: 85,798,656\n",
       "Total mult-adds (Units.MEGABYTES): 172.47\n",
       "=======================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 229.22\n",
       "Estimated Total Size (MB): 333.91\n",
       "======================================================================================================================================="
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = model.heads.head.in_features\n",
    "model.heads = nn.Sequential(\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(in_features= num_features, out_features= NUM_CLASSES, bias=True)\n",
    ").to(device)\n",
    "\n",
    "for param in model.heads.children():\n",
    "    param.requires_grad = True\n",
    "\n",
    "summary(model, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"trainable\"], row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9ce2427f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21afbd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
