{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf4b3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad7e83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "def download_and_extract_zip(url: str, destination: str, remove_source: bool = True) -> Path:\n",
    "\n",
    "    data_folder = Path('data/')\n",
    "    destination_path = data_folder / destination\n",
    "\n",
    "    if data_folder.is_dir():\n",
    "        print(f\"Data folder {data_folder} already exists.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Downloading data from {url} to {data_folder}\")\n",
    "        destination_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        target_file = Path(url).name\n",
    "\n",
    "        with open(data_folder / target_file, 'wb') as f:\n",
    "            response = requests.get(url)\n",
    "            f.write(response.content)\n",
    "        \n",
    "        with zipfile.ZipFile(data_folder / target_file, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(data_folder)\n",
    "        \n",
    "        if remove_source:\n",
    "            os.remove(data_folder / target_file)\n",
    "    \n",
    "    return destination_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a67f96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oak/coding/projects/deep-learning/audio-classification-using-ViT-/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from datasets import load_dataset\n",
    "except:\n",
    "    %pip -q install datasets\n",
    "    from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"danavery/urbansound8K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d16b8b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: dict_keys(['train'])\n"
     ]
    }
   ],
   "source": [
    "available_splits = ds.keys()\n",
    "print(f\"Available splits: {available_splits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13f1878a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None),\n",
       " 'slice_file_name': Value(dtype='string', id=None),\n",
       " 'fsID': Value(dtype='int64', id=None),\n",
       " 'start': Value(dtype='float64', id=None),\n",
       " 'end': Value(dtype='float64', id=None),\n",
       " 'salience': Value(dtype='int64', id=None),\n",
       " 'fold': Value(dtype='int64', id=None),\n",
       " 'classID': Value(dtype='int64', id=None),\n",
       " 'class': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "330d9d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 8732\n",
      "Fold column exists. Splitting dataset into predefined folds.\n",
      "Train dataset size: 7079\n",
      "Validation dataset size: 816\n",
      "Test dataset size: 837\n",
      "Data split successfully.\n"
     ]
    }
   ],
   "source": [
    "if \"train\" in ds.keys():\n",
    "    full_dataset = ds[\"train\"]\n",
    "    print(f\"Full dataset size: {len(full_dataset)}\")\n",
    "\n",
    "    if \"fold\" in full_dataset.features:\n",
    "        print(\"Fold column exists. Splitting dataset into predefined folds.\")\n",
    "\n",
    "        train_dataset = full_dataset.filter(lambda x: x[\"fold\"] <= 8)\n",
    "        val_dataset = full_dataset.filter(lambda x: x[\"fold\"] == 9)\n",
    "        test_dataset = full_dataset.filter(lambda x: x[\"fold\"] == 10)\n",
    "\n",
    "        print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "        print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "        print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "        if len(train_dataset) + len(val_dataset) + len(test_dataset) == len(\n",
    "            full_dataset\n",
    "        ):\n",
    "            print(\"Data split successfully.\")\n",
    "        else:\n",
    "            print(\n",
    "                \"ERROR: Sum of split doesn't match the full dataset. Check the folds again.\"\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: 'fold' column not found. Performing a random split.\")\n",
    "\n",
    "        # Split full dataset\n",
    "        # Training : 80%\n",
    "        # Temp Test: 20%\n",
    "        train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "        train_dataset = train_test_split[\"train\"]\n",
    "        test_temp_dataset = train_test_split[\"test\"]\n",
    "\n",
    "        # Split temp test dataset\n",
    "        # val_dataset: 50%\n",
    "        # test_dataset: 50%\n",
    "        val_test_split = test_temp_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "        val_dataset = val_test_split[\"train\"]\n",
    "        test_dataset = val_test_split[\"test\"]\n",
    "\n",
    "        print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "        print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "        print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: 'train' split not found. Please check the dataset.\")\n",
    "    train_dataset = None\n",
    "    val_dataset = None\n",
    "    test_dataset = None\n",
    "    print(\"The dataset has \", ds.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae883c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 7079\n",
      "Validation dataset size: 816\n",
      "Test dataset size: 837\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '100032-3-0-0.wav',\n",
       "  'array': array([-0.00454712, -0.00483704, -0.00460815, ..., -0.00065613,\n",
       "         -0.00048828,  0.        ], shape=(14004,)),\n",
       "  'sampling_rate': 44100},\n",
       " 'slice_file_name': '100032-3-0-0.wav',\n",
       " 'fsID': 100032,\n",
       " 'start': 0.0,\n",
       " 'end': 0.317551,\n",
       " 'salience': 1,\n",
       " 'fold': 5,\n",
       " 'classID': 3,\n",
       " 'class': 'dog_bark'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "326663bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as TV\n",
    "import numpy as np\n",
    "\n",
    "# 1. Preprocessing function\n",
    "# parameters\n",
    "N_MELS = 128\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "VIT_INPUT_SIZE = (224, 224)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "FREQ_MASK_PARAM = 15  # for SpecAugment\n",
    "TIME_MASK_PARAM = 20  # for SpecAugment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectrogram Calculation\n",
    "mel_spectrogram_transform = T.MelSpectrogram(\n",
    "    sample_rate=TARGET_SAMPLE_RATE,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    n_mels=N_MELS,\n",
    "    power=2.0,\n",
    ")\n",
    "\n",
    "amplitude_to_db_transform = T.AmplitudeToDB(stype=\"power\", top_db=80.0)\n",
    "\n",
    "# SpecAugment (Frequency and Time Masking)\n",
    "freq_mask_transform = T.FrequencyMasking(freq_mask_param=30)\n",
    "time_mask_transform = T.TimeMasking(time_mask_param=40)\n",
    "\n",
    "\n",
    "# # transforms to make it compatible with vision models\n",
    "# custom transform to handle channels\n",
    "class HandleChannels(nn.Module):\n",
    "    def forward(self, spec: torch.Tensor) -> torch.Tensor:\n",
    "        # Input shape: [1, n_mels, time_steps]\n",
    "        # Output shape: [3, n_mels, time_steps]\n",
    "        if spec.ndim == 2:\n",
    "            # If input is 2D, add a channel dimension\n",
    "            spec = spec.unsqueeze(0)\n",
    "        # format now: [1, n_mels, time_steps]\n",
    "        # Repeat across channel dimension to get 3 channels (mimicking RGB)\n",
    "        # If input has 1 channel, repeat to 3 channels\n",
    "        if spec.shape[0] == 1:\n",
    "            spec = spec.repeat(3, 1, 1)\n",
    "        # Shape after: [3, n_mels, time_steps]\n",
    "        return spec\n",
    "\n",
    "\n",
    "handle_channels_transform = HandleChannels()\n",
    "\n",
    "# Vision transforms\n",
    "resize_transform = TV.Resize(VIT_INPUT_SIZE, antialias=True)\n",
    "normalize_transform = TV.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "# Create the transform pipeline\n",
    "\n",
    "eval_transforms = TV.Compose(\n",
    "    [\n",
    "        mel_spectrogram_transform,\n",
    "        amplitude_to_db_transform,\n",
    "        handle_channels_transform,\n",
    "        resize_transform,\n",
    "        normalize_transform,\n",
    "    ]\n",
    ")\n",
    "\n",
    "training_transforms = TV.Compose(\n",
    "    [\n",
    "        mel_spectrogram_transform,\n",
    "        amplitude_to_db_transform,\n",
    "        # These transforms expect (..., freq, time)\n",
    "        freq_mask_transform,\n",
    "        time_mask_transform,\n",
    "        # Image transforms\n",
    "        handle_channels_transform,\n",
    "        resize_transform,\n",
    "        normalize_transform,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8b8fc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '100032-3-0-0.wav',\n",
       "  'array': array([-0.00454712, -0.00483704, -0.00460815, ..., -0.00065613,\n",
       "         -0.00048828,  0.        ], shape=(14004,)),\n",
       "  'sampling_rate': 44100},\n",
       " 'slice_file_name': '100032-3-0-0.wav',\n",
       " 'fsID': 100032,\n",
       " 'start': 0.0,\n",
       " 'end': 0.317551,\n",
       " 'salience': 1,\n",
       " 'fold': 5,\n",
       " 'classID': 3,\n",
       " 'class': 'dog_bark'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc347133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0045, -0.0048, -0.0046,  ..., -0.0007, -0.0005,  0.0000],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(train_dataset[0][\"audio\"][\"array\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb4c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def preprocess_data(is_training: bool):\n",
    "    \"\"\" Returns the function that processes a BATCH dictionary \"\"\"\n",
    "\n",
    "    processor = training_transforms if is_training else eval_transforms\n",
    "\n",
    "    def preprocess_batch(batch: dict) -> dict:\n",
    "        processed_spectrograms = []\n",
    "        labels = []\n",
    "        audio_list = batch[\"audio\"]\n",
    "        classID_list = batch[\"classID\"]\n",
    "        num_items = len(classID_list)\n",
    "\n",
    "        for i in range(num_items):\n",
    "            audio_data = audio_list[i]\n",
    "            waveform = torch.from_numpy(audio_data[\"array\"]).float()\n",
    "            sample_rate = audio_data[\"sampling_rate\"]\n",
    "            label = torch.tensor(classID_list[i], dtype=torch.long)\n",
    "\n",
    "            # Resample if necessary\n",
    "            if sample_rate != TARGET_SAMPLE_RATE:\n",
    "                resampler = T.Resample(orig_freq=sample_rate, new_freq=TARGET_SAMPLE_RATE)\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            # Convert to mono if necessary\n",
    "            if waveform.ndim > 1 and waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0)\n",
    "            if waveform.ndim == 0:\n",
    "                waveform = waveform.unsqueeze(0)\n",
    "\n",
    "            # --- Add Padding for Short Waveforms ---\n",
    "            # Ensure waveform is long enough for the STFT window (n_fft)\n",
    "            # The MelSpectrogram transform itself (via STFT) effectively needs\n",
    "            # the signal to be at least n_fft long for the default centered behavior.\n",
    "            required_len = N_FFT\n",
    "            if waveform.shape[-1] < required_len:\n",
    "                padding_needed = required_len - waveform.shape[-1]\n",
    "                # Pad on the right side (zeros)\n",
    "                waveform = F.pad(waveform, (0, padding_needed))\n",
    "            # --- End of Padding ---\n",
    "\n",
    "            # Apply the Compose pipeline\n",
    "            processed_spectrogram = processor(waveform)\n",
    "\n",
    "            processed_spectrograms.append(processed_spectrogram)\n",
    "            labels.append(label)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": processed_spectrograms,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "    return preprocess_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85f60bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transforms to datasets...\n",
      "Sample before transform: {'audio': {'path': '100032-3-0-0.wav', 'array': array([-0.00454712, -0.00483704, -0.00460815, ..., -0.00065613,\n",
      "       -0.00048828,  0.        ], shape=(14004,)), 'sampling_rate': 44100}, 'slice_file_name': '100032-3-0-0.wav', 'fsID': 100032, 'start': 0.0, 'end': 0.317551, 'salience': 1, 'fold': 5, 'classID': 3, 'class': 'dog_bark'}\n",
      "Transforms applied successfully.\n",
      "Processed sample shape: torch.Size([3, 224, 224])\n",
      "Processed sample label: 3\n",
      "Processed spectrogram type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "train_transform_fn = preprocess_data(is_training=True)\n",
    "val_transform_fn = preprocess_data(is_training=False)\n",
    "\n",
    "print(\"Applying transforms to datasets...\")\n",
    "try:\n",
    "    train_exists = \"train_dataset\" in locals() and train_dataset is not None\n",
    "    val_exists = \"val_dataset\" in locals() and val_dataset is not None\n",
    "    test_exists = \"test_dataset\" in locals() and test_dataset is not None\n",
    "\n",
    "    print(\"Sample before transform:\", train_dataset[0])\n",
    "    if train_exists:\n",
    "        train_dataset.set_transform(train_transform_fn)\n",
    "    if val_exists:\n",
    "        val_dataset.set_transform(val_transform_fn)\n",
    "    if test_exists:\n",
    "        test_dataset.set_transform(val_transform_fn)\n",
    "\n",
    "    if train_exists or val_exists or test_exists:\n",
    "        print(\"Transforms applied successfully.\")\n",
    "    else:\n",
    "        print(\"No datasets found to apply transforms.\")\n",
    "\n",
    "    if train_exists:\n",
    "        processed_sample = train_dataset[0]\n",
    "        print(f\"Processed sample shape: {processed_sample['pixel_values'].shape}\")\n",
    "        print(f\"Processed sample label: {processed_sample['labels']}\")\n",
    "        print(f\"Processed spectrogram type: {processed_sample['pixel_values'].dtype}\")\n",
    "# ...existing code...\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error applying transforms: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab11872f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of train_dataset: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Type of train_dataset[0]: <class 'dict'>\n",
      "train_dataset[0]: {'pixel_values': tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 'labels': tensor(3)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of train_dataset:\", type(train_dataset))\n",
    "first = train_dataset[0]\n",
    "print(\"Type of train_dataset[0]:\", type(first))\n",
    "print(\"train_dataset[0]:\", first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bccfaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataloader size: 222\n",
      "Validation dataloader size: 26\n",
      "Test dataloader size: 27\n"
     ]
    }
   ],
   "source": [
    "# Crate DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "print(f\"Train dataloader size: {len(train_dataloader)}\")\n",
    "print(f\"Validation dataloader size: {len(val_dataloader)}\")\n",
    "print(f\"Test dataloader size: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93c59646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape              Param #                   Trainable\n",
       "================================================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]          [1, 1000]                 768                       True\n",
       "├─Conv2d (conv_proj)                                         [1, 3, 224, 224]          [1, 768, 14, 14]          590,592                   True\n",
       "├─Encoder (encoder)                                          [1, 197, 768]             [1, 197, 768]             151,296                   True\n",
       "│    └─Dropout (dropout)                                     [1, 197, 768]             [1, 197, 768]             --                        --\n",
       "│    └─Sequential (layers)                                   [1, 197, 768]             [1, 197, 768]             --                        True\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]             [1, 197, 768]             7,087,872                 True\n",
       "│    └─LayerNorm (ln)                                        [1, 197, 768]             [1, 197, 768]             1,536                     True\n",
       "├─Sequential (heads)                                         [1, 768]                  [1, 1000]                 --                        True\n",
       "│    └─Linear (head)                                         [1, 768]                  [1, 1000]                 769,000                   True\n",
       "================================================================================================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 173.23\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 232.27\n",
       "Estimated Total Size (MB): 336.96\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchinfo import summary\n",
    "# Create model\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "model = torchvision.models.vit_b_16(weights=weights)\n",
    "\n",
    "summary(model, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "443e8e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv_proj, Module: Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "Layer: encoder, Module: Encoder(\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): Sequential(\n",
      "    (encoder_layer_0): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_1): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_2): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_3): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_4): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_5): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_6): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_7): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_8): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_9): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_10): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_11): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "Layer: heads, Module: Sequential(\n",
      "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for layer in model.parameters():\n",
    "    layer.requires_grad = False\n",
    "\n",
    "for name, module in list(model.named_children())[-3:]:\n",
    "    print(f\"Layer: {name}, Module: {module}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52fedc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa7cc582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape              Trainable\n",
       "=======================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]          [1, 10]                   Partial\n",
       "├─Conv2d (conv_proj)                                         [1, 3, 224, 224]          [1, 768, 14, 14]          False\n",
       "├─Encoder (encoder)                                          [1, 197, 768]             [1, 197, 768]             False\n",
       "│    └─Dropout (dropout)                                     [1, 197, 768]             [1, 197, 768]             --\n",
       "│    └─Sequential (layers)                                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]             [1, 197, 768]             False\n",
       "│    └─LayerNorm (ln)                                        [1, 197, 768]             [1, 197, 768]             False\n",
       "├─Sequential (heads)                                         [1, 768]                  [1, 10]                   True\n",
       "│    └─Dropout (0)                                           [1, 768]                  [1, 768]                  --\n",
       "│    └─Linear (1)                                            [1, 768]                  [1, 10]                   True\n",
       "=======================================================================================================================================\n",
       "Total params: 85,806,346\n",
       "Trainable params: 7,690\n",
       "Non-trainable params: 85,798,656\n",
       "Total mult-adds (Units.MEGABYTES): 172.47\n",
       "=======================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 229.22\n",
       "Estimated Total Size (MB): 333.91\n",
       "======================================================================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = model.heads.head.in_features\n",
    "model.heads = nn.Sequential(\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(in_features= num_features, out_features= NUM_CLASSES, bias=True)\n",
    ").to(device)\n",
    "\n",
    "for param in model.heads.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "summary(model, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"trainable\"], row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55fc5b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blocks in encoder (Zero-Indexed): 12\n",
      "Unfreezing layer 6: EncoderBlock(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (4): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Unfreezing layer 7: EncoderBlock(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (4): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Unfreezing layer 8: EncoderBlock(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (4): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Unfreezing layer 9: EncoderBlock(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (4): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Unfreezing layer 10: EncoderBlock(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (4): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Unfreezing layer 11: EncoderBlock(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (4): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape              Trainable\n",
       "=======================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]          [1, 10]                   Partial\n",
       "├─Conv2d (conv_proj)                                         [1, 3, 224, 224]          [1, 768, 14, 14]          False\n",
       "├─Encoder (encoder)                                          [1, 197, 768]             [1, 197, 768]             Partial\n",
       "│    └─Dropout (dropout)                                     [1, 197, 768]             [1, 197, 768]             --\n",
       "│    └─Sequential (layers)                                   [1, 197, 768]             [1, 197, 768]             Partial\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]             [1, 197, 768]             False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]             [1, 197, 768]             True\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]             [1, 197, 768]             True\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]             [1, 197, 768]             True\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]             [1, 197, 768]             True\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]             [1, 197, 768]             True\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]             [1, 197, 768]             True\n",
       "│    └─LayerNorm (ln)                                        [1, 197, 768]             [1, 197, 768]             False\n",
       "├─Sequential (heads)                                         [1, 768]                  [1, 10]                   True\n",
       "│    └─Dropout (0)                                           [1, 768]                  [1, 768]                  --\n",
       "│    └─Linear (1)                                            [1, 768]                  [1, 10]                   True\n",
       "=======================================================================================================================================\n",
       "Total params: 85,806,346\n",
       "Trainable params: 42,534,922\n",
       "Non-trainable params: 43,271,424\n",
       "Total mult-adds (Units.MEGABYTES): 172.47\n",
       "=======================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 229.22\n",
       "Estimated Total Size (MB): 333.91\n",
       "======================================================================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of blocks in encoder (Zero-Indexed):\", len(model.encoder.layers))\n",
    "num_of_layers_to_unfreeze = len(model.encoder.layers) // 2\n",
    "\n",
    "for i, layer in enumerate(model.encoder.layers[-1 * num_of_layers_to_unfreeze:]):\n",
    "    layer_num = len(model.encoder.layers) - num_of_layers_to_unfreeze + i\n",
    "    print(f\"Unfreezing layer {layer_num}: {layer}\")\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    input_size=(1, 3, 224, 224),\n",
    "    col_names=[\"input_size\", \"output_size\", \"trainable\"],\n",
    "    row_settings=[\"var_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ce2427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2cd8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": model.encoder.layers.parameters(), \"lr\": 1e-6},\n",
    "        {\"params\": model.heads.parameters(), \"lr\": 1e-3},\n",
    "    ],\n",
    "    weight_decay=1e-2,\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9c50d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# check model device\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21afbd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:33<14:03, 93.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.8366 | train_acc: 0.3189 | test_loss: 1.6938 | test_acc: 0.3634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [03:06<12:27, 93.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 1.5678 | train_acc: 0.4266 | test_loss: 1.5125 | test_acc: 0.4861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [04:40<10:53, 93.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 1.4579 | train_acc: 0.4647 | test_loss: 1.4130 | test_acc: 0.5150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [06:13<09:18, 93.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 1.3945 | train_acc: 0.4992 | test_loss: 1.3723 | test_acc: 0.5366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [07:46<07:46, 93.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 1.3340 | train_acc: 0.5206 | test_loss: 1.2577 | test_acc: 0.5852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [09:20<06:13, 93.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss: 1.2867 | train_acc: 0.5409 | test_loss: 1.2919 | test_acc: 0.5794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [10:53<04:39, 93.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | train_loss: 1.2413 | train_acc: 0.5551 | test_loss: 1.2751 | test_acc: 0.5840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored from cffi callback <function SoundFile._init_virtual_io.<locals>.vio_read at 0x7fdb5e1fdda0>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oak/coding/projects/deep-learning/audio-classification-using-ViT-/venv/lib/python3.12/site-packages/soundfile.py\", line 1290, in vio_read\n",
      "    @_ffi.callback(\"sf_vio_read\")\n",
      ": \n",
      "KeyboardInterrupt\n",
      " 70%|███████   | 7/10 [11:24<04:53, 97.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensorboard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[32m      4\u001b[39m writer = SummaryWriter(\u001b[33m\"\u001b[39m\u001b[33mruns/urbansound8k\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model_results = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/projects/deep-learning/audio-classification-using-ViT-/modules/going_modular/engine.py:161\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device, writer, save_dir, save_n_epoch)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     train_loss, train_acc = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m     test_loss, test_acc = test_step(\n\u001b[32m    169\u001b[39m         model=model, dataloader=test_dataloader, loss_fn=loss_fn, device=device\n\u001b[32m    170\u001b[39m     )\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# Print out what's happening\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/projects/deep-learning/audio-classification-using-ViT-/modules/going_modular/engine.py:58\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(model, dataloader, loss_fn, optimizer, device)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# 2. Calculate  and accumulate loss\u001b[39;00m\n\u001b[32m     57\u001b[39m loss = loss_fn(y_pred, y)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# 3. Optimizer zero grad\u001b[39;00m\n\u001b[32m     61\u001b[39m optimizer.zero_grad()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from modules.going_modular import engine\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"runs/urbansound8k\")\n",
    "\n",
    "model_results = engine.train(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=num_epochs,\n",
    "    device=device,\n",
    "    writer=writer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator for the train_dataloader\n",
    "train_iterator = iter(train_dataloader)\n",
    "\n",
    "# Get the first batch from the iterator\n",
    "first_batch = next(train_iterator)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(first_batch[\"pixel_values\"][3].permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b8190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset as HFDataset # Import for type hinting\n",
    "import time\n",
    "\n",
    "# --- Make sure Parameters are defined globally or pass them ---\n",
    "N_MELS = 128\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "\n",
    "# --- Define necessary Transforms (can be global) ---\n",
    "mel_spectrogram_viz_transform = T.MelSpectrogram(\n",
    "    sample_rate=TARGET_SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS, power=2.0\n",
    ")\n",
    "amplitude_to_db_viz_transform = T.AmplitudeToDB(stype='power', top_db=80.0)\n",
    "\n",
    "\n",
    "def display_unprocessed_spectrogram(dataset: HFDataset, index: int):\n",
    "    \"\"\"\n",
    "    Loads a sample from the dataset (temporarily bypassing set_transform),\n",
    "    calculates its dB Mel spectrogram, and displays it using librosa.\n",
    "\n",
    "    Args:\n",
    "        dataset: The Hugging Face Dataset object (e.g., train_dataset).\n",
    "        index: The integer index of the sample to display.\n",
    "    \"\"\"\n",
    "    if not isinstance(index, int) or index < 0 or index >= len(dataset):\n",
    "        print(f\"Error: Index {index} is out of bounds for dataset size {len(dataset)}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Generating spectrogram for sample index: {index}\")\n",
    "\n",
    "    # --- Get the raw sample using set_format(None) ---\n",
    "    try:\n",
    "        # Temporarily set format to None to bypass python function transforms\n",
    "        dataset.set_format(type=None)\n",
    "        print(f\"Fetching raw sample at index {index}...\")\n",
    "        raw_sample = dataset[index] # Get the raw dictionary\n",
    "        print(\"Raw sample fetched successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting raw sample at index {index}: {e}\")\n",
    "        # Ensure format is reset even if fetching fails\n",
    "        dataset.reset_format()\n",
    "        print(\"Dataset format reset after error.\")\n",
    "        return\n",
    "    finally:\n",
    "        # --- IMPORTANT: Reset format back to default ---\n",
    "        # This allows subsequent calls (e.g., by DataLoader) to use the transform\n",
    "        # that was previously set by set_transform.\n",
    "        dataset.reset_format()\n",
    "        print(\"Dataset format reset.\")\n",
    "\n",
    "\n",
    "    # --- Basic Validation ---\n",
    "    if not isinstance(raw_sample, dict) or 'audio' not in raw_sample or not isinstance(raw_sample['audio'], dict):\n",
    "         print(f\"Error: Sample at index {index} does not have the expected structure.\")\n",
    "         return\n",
    "\n",
    "    # --- Extract and Preprocess Audio ---\n",
    "    try:\n",
    "        print(\"Processing audio...\")\n",
    "        audio_data = raw_sample['audio']\n",
    "        waveform = torch.from_numpy(audio_data['array']).float()\n",
    "        sample_rate = audio_data['sampling_rate']\n",
    "        class_name = raw_sample.get('class', 'N/A')\n",
    "\n",
    "        # 1. Resample if necessary\n",
    "        if sample_rate != TARGET_SAMPLE_RATE:\n",
    "            # Create resampler instance *only when needed*\n",
    "            resampler = T.Resample(orig_freq=sample_rate, new_freq=TARGET_SAMPLE_RATE)\n",
    "            waveform = resampler(waveform)\n",
    "            sample_rate = TARGET_SAMPLE_RATE # Use target rate from now on\n",
    "\n",
    "        # 2. Convert to mono if necessary\n",
    "        if waveform.ndim > 1 and waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0)\n",
    "        if waveform.ndim == 0:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "\n",
    "        # 3. Pad if necessary\n",
    "        required_len = N_FFT\n",
    "        original_len = waveform.shape[-1]\n",
    "        if original_len < required_len:\n",
    "            padding_needed = required_len - original_len\n",
    "            waveform = F.pad(waveform, (0, padding_needed))\n",
    "            # print(f\"Info: Padded waveform from {original_len} to {waveform.shape[-1]} samples.\") # Optional debug\n",
    "\n",
    "        # 4. Calculate dB Mel Spectrogram\n",
    "        mel_spec_tensor = mel_spectrogram_viz_transform(waveform)\n",
    "        db_mel_spec_tensor = amplitude_to_db_viz_transform(mel_spec_tensor)\n",
    "\n",
    "        # 5. Convert for plotting\n",
    "        db_mel_spectrogram_np = db_mel_spec_tensor.squeeze().cpu().numpy()\n",
    "        print(\"Audio processed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio for sample {index}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # --- Plotting ---\n",
    "    print(f\"Plotting spectrogram shape: {db_mel_spectrogram_np.shape}\")\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    try:\n",
    "        img = librosa.display.specshow(\n",
    "            db_mel_spectrogram_np,\n",
    "            sr=sample_rate, # Should be TARGET_SAMPLE_RATE\n",
    "            hop_length=HOP_LENGTH,\n",
    "            x_axis='time',\n",
    "            y_axis='mel',\n",
    "            cmap='gray'\n",
    "        )\n",
    "        plt.colorbar(img, format='%+2.0f dB')\n",
    "        plt.title(f'Unprocessed dB Mel Spectrogram (Sample {index})\\nClass: {class_name}')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Frequency (Mel)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"Plot displayed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during plotting: {e}\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Make sure train_dataset is loaded and valid\n",
    "if 'train_dataset' in locals() and train_dataset is not None:\n",
    "    # It's good practice to ensure the main transform is set before viewing\n",
    "    # Or handle cases where it might not be set yet\n",
    "    # if train_dataset.transform is None: # Check if transform attribute exists and is None (more robust needed if attr doesn't exist)\n",
    "    try:\n",
    "        print(\"Applying training transform before viewing...\")\n",
    "        train_transform_fn = preprocess_data(is_training=True) # Ensure this is defined\n",
    "        train_dataset.set_transform(train_transform_fn)\n",
    "    except NameError:\n",
    "        print(\"Warning: train_transform_fn not defined. Cannot set transform.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to set transform - {e}\")\n",
    "\n",
    "    display_unprocessed_spectrogram(train_dataset, 15) # Display sample index 15\n",
    "    display_unprocessed_spectrogram(train_dataset, 1702) # Display another sample\n",
    "\n",
    "    # Verify transform is still active after viewing (it should be due to reset_format)\n",
    "    # print(\"\\nVerifying transform is still active...\")\n",
    "    # try:\n",
    "    #    sample_after = train_dataset[0]\n",
    "    #    print(\"Keys after viewing:\", sample_after.keys()) # Should be pixel_values, labels\n",
    "    # except Exception as e:\n",
    "    #    print(f\"Error accessing sample after viewing: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"train_dataset not found. Cannot display spectrogram.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b8b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as TV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset as HFDataset # Import for type hinting\n",
    "import random\n",
    "import traceback # For printing full tracebacks on error\n",
    "\n",
    "# --- Ensure Parameters are defined ---\n",
    "N_MELS = 128\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "VIT_INPUT_SIZE = (224, 224)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "FREQ_MASK_PARAM = 15 # Use the values you set\n",
    "TIME_MASK_PARAM = 20 # Use the values you set\n",
    "\n",
    "# --- Ensure Reusable Transform Components are defined ---\n",
    "mel_spectrogram_transform = T.MelSpectrogram(\n",
    "    sample_rate=TARGET_SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS, power=2.0,\n",
    ")\n",
    "amplitude_to_db_transform = T.AmplitudeToDB(stype=\"power\", top_db=80.0)\n",
    "freq_mask_transform = T.FrequencyMasking(freq_mask_param=FREQ_MASK_PARAM)\n",
    "time_mask_transform = T.TimeMasking(time_mask_param=TIME_MASK_PARAM)\n",
    "class HandleChannels(nn.Module):\n",
    "     def forward(self, spec: torch.Tensor) -> torch.Tensor:\n",
    "         if spec.ndim == 2: spec = spec.unsqueeze(0)\n",
    "         if spec.shape[0] == 1: spec = spec.repeat(3, 1, 1)\n",
    "         return spec\n",
    "handle_channels_transform = HandleChannels()\n",
    "resize_transform = TV.Resize(VIT_INPUT_SIZE, antialias=True)\n",
    "normalize_transform = TV.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "# --- Unnormalize Function ---\n",
    "def unnormalize(tensor, mean, std, inplace=False):\n",
    "    if not inplace: tensor = tensor.clone()\n",
    "    mean = torch.as_tensor(mean, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
    "    std = torch.as_tensor(std, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
    "    tensor.mul_(std).add_(mean)\n",
    "    tensor = torch.clamp(tensor, 0., 1.)\n",
    "    return tensor\n",
    "\n",
    "# --- CORRECTED Visualization Function ---\n",
    "def visualize_transform_stages(dataset: HFDataset, index: int, is_training: bool):\n",
    "    \"\"\"\n",
    "    Visualizes key stages of the spectrogram transformation for a single sample.\n",
    "    Uses set_format/reset_format to get raw data safely.\n",
    "    \"\"\"\n",
    "    # --- Parameter / Transform Check ---\n",
    "    # (Keep checks for required globals if you prefer)\n",
    "\n",
    "    if not isinstance(index, int) or index < 0 or index >= len(dataset):\n",
    "        print(f\"Error: Index {index} is out of bounds for dataset size {len(dataset)}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Visualizing Transform Stages for Sample Index: {index} ---\")\n",
    "    print(f\"Mode: {'TRAINING (with Augmentation)' if is_training else 'EVALUATION (no Augmentation)'}\")\n",
    "\n",
    "    # --- Get Raw Sample (Safely using set_format/reset_format) --- # CORRECTED BLOCK\n",
    "    raw_sample = None\n",
    "    try:\n",
    "        # Temporarily set format to None to bypass python function transforms\n",
    "        # This effectively makes dataset[index] return raw python dict\n",
    "        dataset.set_format(type=None)\n",
    "        raw_sample = dataset[index]\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting raw sample at index {index}: {e}\")\n",
    "        dataset.reset_format() # Ensure format is reset even if fetching fails\n",
    "        return\n",
    "    finally:\n",
    "        # --- IMPORTANT: Reset format back to default ---\n",
    "        # This ensures the dataset behaves as expected (with set_transform) afterwards\n",
    "        dataset.reset_format()\n",
    "    # --- END CORRECTED BLOCK ---\n",
    "\n",
    "\n",
    "    # --- Basic Validation ---\n",
    "    if not isinstance(raw_sample, dict) or 'audio' not in raw_sample or not isinstance(raw_sample['audio'], dict):\n",
    "         print(f\"Error: Sample at index {index} does not have the expected structure ({type(raw_sample)}).\")\n",
    "         return\n",
    "\n",
    "    # --- Store intermediate results ---\n",
    "    stages = {}\n",
    "\n",
    "    # --- Extract and Preprocess Waveform ---\n",
    "    try:\n",
    "        audio_data = raw_sample['audio']\n",
    "        waveform = torch.from_numpy(audio_data['array']).float()\n",
    "        sample_rate = audio_data['sampling_rate']\n",
    "        class_name = raw_sample.get('class', 'N/A')\n",
    "\n",
    "        # Resample\n",
    "        if sample_rate != TARGET_SAMPLE_RATE:\n",
    "            resampler = T.Resample(orig_freq=sample_rate, new_freq=TARGET_SAMPLE_RATE)\n",
    "            waveform = resampler(waveform)\n",
    "            sample_rate = TARGET_SAMPLE_RATE # Now fixed\n",
    "        # Mono\n",
    "        if waveform.ndim > 1 and waveform.shape[0] > 1: waveform = torch.mean(waveform, dim=0)\n",
    "        if waveform.ndim == 0: waveform = waveform.unsqueeze(0)\n",
    "        # Pad\n",
    "        required_len = N_FFT\n",
    "        if waveform.shape[-1] < required_len:\n",
    "            padding_needed = required_len - waveform.shape[-1]\n",
    "            waveform = F.pad(waveform, (0, padding_needed))\n",
    "        processed_waveform = waveform.clone() # Use this for both paths\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing waveform: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # --- Apply Transforms Step-by-Step for Visualization ---\n",
    "    try:\n",
    "        # 1. dB Mel Spectrogram\n",
    "        mel_spec = mel_spectrogram_transform(processed_waveform)\n",
    "        db_mel_spec = amplitude_to_db_transform(mel_spec)\n",
    "        stages['dB Mel Spectrogram'] = db_mel_spec.squeeze().cpu().numpy()\n",
    "\n",
    "        # 2. After SpecAugment (if training)\n",
    "        current_spec_for_processing = db_mel_spec # Input for next steps\n",
    "        if is_training:\n",
    "            augmented_spec = current_spec_for_processing.clone() # Augment a copy\n",
    "            augmented_spec = freq_mask_transform(augmented_spec)\n",
    "            augmented_spec = time_mask_transform(augmented_spec)\n",
    "            stages['After SpecAugment'] = augmented_spec.squeeze().cpu().numpy()\n",
    "            current_spec_for_processing = augmented_spec # Use augmented spec for final steps\n",
    "        else:\n",
    "            stages['After SpecAugment'] = None # Placeholder if not training\n",
    "\n",
    "        # 3. Final Processed Tensor (apply remaining steps)\n",
    "        remaining_transforms = TV.Compose([\n",
    "             handle_channels_transform, resize_transform, normalize_transform\n",
    "        ])\n",
    "        final_normalized_tensor = remaining_transforms(current_spec_for_processing)\n",
    "\n",
    "        # 4. Unnormalize final tensor for plotting\n",
    "        final_unnormalized = unnormalize(final_normalized_tensor, IMAGENET_MEAN, IMAGENET_STD)\n",
    "        stages['Final Processed (Unnormalized)'] = final_unnormalized.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying transforms step-by-step: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # --- Plotting ---\n",
    "    num_plots = 4 if is_training and stages['After SpecAugment'] is not None else 3\n",
    "    fig, axs = plt.subplots(1, num_plots, figsize=(num_plots * 6, 5))\n",
    "    # Ensure axs is always iterable, even if num_plots is 1 (though unlikely here)\n",
    "    if num_plots == 1: axs = [axs]\n",
    "    fig.suptitle(f'Transformation Stages: Sample {index} (Class: {class_name}) | Mode: {\"TRAIN\" if is_training else \"EVAL\"}', fontsize=16)\n",
    "\n",
    "    plot_idx = 0\n",
    "\n",
    "    # Plot 1: dB Mel Spectrogram\n",
    "    try:\n",
    "        ax = axs[plot_idx]\n",
    "        img = librosa.display.specshow(stages['dB Mel Spectrogram'], sr=TARGET_SAMPLE_RATE, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel', cmap='viridis', ax=ax)\n",
    "        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "        ax.set_title('1. dB Mel Spectrogram')\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Frequency (Mel)')\n",
    "        plot_idx += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting dB Mel Spec: {e}\")\n",
    "        if plot_idx < len(axs): axs[plot_idx].set_title('Error plotting dB Mel Spec'); plot_idx+=1\n",
    "\n",
    "\n",
    "    # Plot 2: After SpecAugment (only if training and available)\n",
    "    if is_training and stages['After SpecAugment'] is not None:\n",
    "        try:\n",
    "            ax = axs[plot_idx]\n",
    "            img = librosa.display.specshow(stages['After SpecAugment'], sr=TARGET_SAMPLE_RATE, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel', cmap='viridis', ax=ax)\n",
    "            ax.set_title('2. After SpecAugment')\n",
    "            ax.set_xlabel('Time (s)')\n",
    "            ax.set_ylabel('Frequency (Mel)')\n",
    "            plot_idx += 1\n",
    "        except Exception as e:\n",
    "             print(f\"Error plotting Augmented Spec: {e}\")\n",
    "             if plot_idx < len(axs): axs[plot_idx].set_title('Error plotting Aug Spec'); plot_idx+=1\n",
    "\n",
    "\n",
    "    # Plot 3: Final Processed (Unnormalized)\n",
    "    try:\n",
    "        ax = axs[plot_idx]\n",
    "        ax.imshow(stages['Final Processed (Unnormalized)'])\n",
    "        final_title_num = plot_idx + 1\n",
    "        ax.set_title(f'{final_title_num}. Final Processed (Unnormalized)')\n",
    "        ax.axis('off')\n",
    "    except Exception as e:\n",
    "         print(f\"Error plotting Final Unnormalized: {e}\")\n",
    "         if plot_idx < len(axs): axs[plot_idx].set_title('Error plotting Final')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.93])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Example Usage (remains the same) ---\n",
    "if 'train_dataset' in locals() and train_dataset is not None:\n",
    "    visualize_transform_stages(train_dataset, 15, is_training=True)\n",
    "    visualize_transform_stages(train_dataset, 15, is_training=False)\n",
    "else:\n",
    "    print(\"train_dataset not found. Cannot run visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b46aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
